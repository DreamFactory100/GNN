{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DreamFactory100/GNN/blob/main/CiteSeer_MLP_GNN_Compare.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# scipy 버젼이 낮으면.. 다음 셀 그래프에서 에러 발생\n",
        "# !pip install 'scipy>=1.8'\n",
        "!pip install --upgrade scipy\n",
        "# ! pip list scipy\n",
        "# import scipy as sp"
      ],
      "metadata": {
        "id": "0uC91hk_v2X3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Node Classification with Graph Neural Networks\n",
        "#\n",
        "# Link : https://keras.io/examples/graph/gnn_citations/\n",
        "#\n",
        "# 노드(논문)의 feature ( 사용된 단어 ) 만을 사용한 MLP 와\n",
        "# 엣지(인용)의 feature ( 인용 관계 )를 포함한 GCN 비교\n",
        "#\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "3tZAYbQwv3mb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zip_file = keras.utils.get_file(\n",
        "    fname=\"cora.tgz\",\n",
        "    origin=\"https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz\",\n",
        "    extract=True,\n",
        ")\n",
        "data_dir = os.path.join(os.path.dirname(zip_file), \"cora\")"
      ],
      "metadata": {
        "id": "XxO2I7Tm0eGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "citations = pd.read_csv(\n",
        "    os.path.join(data_dir, \"cora.cites\"),\n",
        "    sep=\"\\t\",\n",
        "    header=None,\n",
        "    names=[\"target\", \"source\"],\n",
        ")\n",
        "print(\"Citations shape:\", citations.shape)"
      ],
      "metadata": {
        "id": "-ZXrN-OF0mZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "citations.sample(frac=1).head()"
      ],
      "metadata": {
        "id": "w25iM7T20p33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "column_names = [\"paper_id\"] + [f\"term_{idx}\" for idx in range(1433)] + [\"subject\"]\n",
        "papers = pd.read_csv(\n",
        "    os.path.join(data_dir, \"cora.content\"), sep=\"\\t\", header=None, names=column_names,\n",
        ")\n",
        "print(\"Papers shape:\", papers.shape)"
      ],
      "metadata": {
        "id": "BcHOgpUz0sU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(papers.sample(5).T)"
      ],
      "metadata": {
        "id": "acdajPM00unr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(papers.subject.value_counts())"
      ],
      "metadata": {
        "id": "lBVSMmEj0zvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_values = sorted(papers[\"subject\"].unique())\n",
        "class_idx = {name: id for id, name in enumerate(class_values)}\n",
        "paper_idx = {name: idx for idx, name in enumerate(sorted(papers[\"paper_id\"].unique()))}\n",
        "\n",
        "papers[\"paper_id\"] = papers[\"paper_id\"].apply(lambda name: paper_idx[name])\n",
        "citations[\"source\"] = citations[\"source\"].apply(lambda name: paper_idx[name])\n",
        "citations[\"target\"] = citations[\"target\"].apply(lambda name: paper_idx[name])\n",
        "papers[\"subject\"] = papers[\"subject\"].apply(lambda value: class_idx[value])"
      ],
      "metadata": {
        "id": "0aM0sqZ006HC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "colors = papers[\"subject\"].tolist()\n",
        "\n",
        "some_citation = citations.sample(n=1500)\n",
        "print(\"some_citation == \", some_citation)\n",
        "\n",
        "cora_graph = nx.from_pandas_edgelist(some_citation,\n",
        "                                     source = 'source',\n",
        "                                     target =  'target',\n",
        "                                     edge_attr=None,\n",
        "                                     create_using=nx.DiGraph())\n",
        "\n",
        "# cora_graph = nx.from_pandas_edgelist(citations.sample(n=1500))\n",
        "\n",
        "subjects = list(papers[papers[\"paper_id\"].isin(list(cora_graph.nodes))][\"subject\"])\n",
        "nx.draw_spring(cora_graph, node_size=15, node_color=subjects)"
      ],
      "metadata": {
        "id": "mXBIvTxf08u0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"cora_graph.is_directed == \", cora_graph.is_directed())"
      ],
      "metadata": {
        "id": "rbwsELri3n_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data = [], []\n",
        "\n",
        "for _, group_data in papers.groupby(\"subject\"):\n",
        "    # Select around 50% of the dataset for training.\n",
        "    random_selection = np.random.rand(len(group_data.index)) <= 0.5\n",
        "    train_data.append(group_data[random_selection])\n",
        "    test_data.append(group_data[~random_selection])\n",
        "\n",
        "train_data = pd.concat(train_data).sample(frac=1)\n",
        "test_data = pd.concat(test_data).sample(frac=1)\n",
        "\n",
        "print(\"Train data shape:\", train_data.shape)\n",
        "print(\"Test data shape:\", test_data.shape)"
      ],
      "metadata": {
        "id": "3I4bfGPm0_nF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_units = [32, 32]\n",
        "learning_rate = 0.01\n",
        "dropout_rate = 0.5\n",
        "num_epochs = 300\n",
        "batch_size = 256"
      ],
      "metadata": {
        "id": "YLGNIbPQ4Eqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiment(model, x_train, y_train):\n",
        "    # Compile the model.\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate),\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")],\n",
        "    )\n",
        "    # Create an early stopping callback.\n",
        "    early_stopping = keras.callbacks.EarlyStopping(\n",
        "        monitor=\"val_acc\", patience=50, restore_best_weights=True\n",
        "    )\n",
        "    # Fit the model.\n",
        "    history = model.fit(\n",
        "        x=x_train,\n",
        "        y=y_train,\n",
        "        epochs=num_epochs,\n",
        "        batch_size=batch_size,\n",
        "        validation_split=0.15,\n",
        "        callbacks=[early_stopping],\n",
        "    )\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "vcgv-tiq4I24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_learning_curves(history):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    ax1.plot(history.history[\"loss\"])\n",
        "    ax1.plot(history.history[\"val_loss\"])\n",
        "    ax1.legend([\"train\", \"test\"], loc=\"upper right\")\n",
        "    ax1.set_xlabel(\"Epochs\")\n",
        "    ax1.set_ylabel(\"Loss\")\n",
        "\n",
        "    ax2.plot(history.history[\"acc\"])\n",
        "    ax2.plot(history.history[\"val_acc\"])\n",
        "    ax2.legend([\"train\", \"test\"], loc=\"upper right\")\n",
        "    ax2.set_xlabel(\"Epochs\")\n",
        "    ax2.set_ylabel(\"Accuracy\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "QVRi4rCH4LSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_ffn(hidden_units, dropout_rate, name=None):\n",
        "    fnn_layers = []\n",
        "\n",
        "    for units in hidden_units:\n",
        "        fnn_layers.append(layers.BatchNormalization())\n",
        "        fnn_layers.append(layers.Dropout(dropout_rate))\n",
        "        fnn_layers.append(layers.Dense(units, activation=tf.nn.gelu))\n",
        "\n",
        "    return keras.Sequential(fnn_layers, name=name)"
      ],
      "metadata": {
        "id": "F5aFwQ9i4O6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names = set(papers.columns) - {\"paper_id\", \"subject\"}\n",
        "print(\"입력 feature names == \", feature_names)\n",
        "num_features = len(feature_names)\n",
        "num_classes = len(class_idx)\n",
        "\n",
        "# Create train and test features as a numpy array.\n",
        "x_train = train_data[feature_names].to_numpy()\n",
        "x_test = test_data[feature_names].to_numpy()\n",
        "# Create train and test targets as a numpy array.\n",
        "y_train = train_data[\"subject\"]\n",
        "y_test = test_data[\"subject\"]"
      ],
      "metadata": {
        "id": "JtcLgbYO4Ss8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_baseline_model(hidden_units, num_classes, dropout_rate=0.2):\n",
        "    inputs = layers.Input(shape=(num_features,), name=\"input_features\")\n",
        "    x = create_ffn(hidden_units, dropout_rate, name=f\"ffn_block1\")(inputs)\n",
        "    for block_idx in range(4):\n",
        "        # Create an FFN block.\n",
        "        x1 = create_ffn(hidden_units, dropout_rate, name=f\"ffn_block{block_idx + 2}\")(x)\n",
        "        # Add skip connection.\n",
        "        x = layers.Add(name=f\"skip_connection{block_idx + 2}\")([x, x1])\n",
        "    # Compute logits.\n",
        "    logits = layers.Dense(num_classes, name=\"logits\")(x)\n",
        "    # Create the model.\n",
        "    return keras.Model(inputs=inputs, outputs=logits, name=\"baseline\")\n",
        "\n",
        "\n",
        "baseline_model = create_baseline_model(hidden_units, num_classes, dropout_rate)\n",
        "baseline_model.summary()"
      ],
      "metadata": {
        "id": "7oT9Q6-84XQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = run_experiment(baseline_model, x_train, y_train)"
      ],
      "metadata": {
        "id": "zhJXqYnv4bBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_learning_curves(history)"
      ],
      "metadata": {
        "id": "JbsrzJBj4ihY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_, test_accuracy = baseline_model.evaluate(x=x_test, y=y_test, verbose=0)\n",
        "print(f\"Test accuracy: {round(test_accuracy * 100, 2)}%\")"
      ],
      "metadata": {
        "id": "iKr0AjwE4nEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_random_instances(num_instances):\n",
        "    token_probability = x_train.mean(axis=0)\n",
        "    instances = []\n",
        "    for _ in range(num_instances):\n",
        "        probabilities = np.random.uniform(size=len(token_probability))\n",
        "        instance = (probabilities <= token_probability).astype(int)\n",
        "        instances.append(instance)\n",
        "\n",
        "    return np.array(instances)\n",
        "\n",
        "\n",
        "def display_class_probabilities(probabilities):\n",
        "    for instance_idx, probs in enumerate(probabilities):\n",
        "        print(f\"Instance {instance_idx + 1}:\")\n",
        "        for class_idx, prob in enumerate(probs):\n",
        "            print(f\"- {class_values[class_idx]}: {round(prob * 100, 2)}%\")"
      ],
      "metadata": {
        "id": "gcmRuRQL4rnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_instances = generate_random_instances(num_classes)\n",
        "logits = baseline_model.predict(new_instances)\n",
        "probabilities = keras.activations.softmax(tf.convert_to_tensor(logits)).numpy()\n",
        "display_class_probabilities(probabilities)"
      ],
      "metadata": {
        "id": "iBgSHAvB4urw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an edges array (sparse adjacency matrix) of shape [2, num_edges].\n",
        "edges = citations[[\"source\", \"target\"]].to_numpy().T\n",
        "# Create an edge weights array of ones.\n",
        "edge_weights = tf.ones(shape=edges.shape[1])\n",
        "# Create a node features array of shape [num_nodes, num_features].\n",
        "node_features = tf.cast(\n",
        "    papers.sort_values(\"paper_id\")[feature_names].to_numpy(), dtype=tf.dtypes.float32\n",
        ")\n",
        "# Create graph info tuple with node_features, edges, and edge_weights.\n",
        "graph_info = (node_features, edges, edge_weights)\n",
        "\n",
        "print(\"Edges shape:\", edges.shape)\n",
        "print(\"Nodes shape:\", node_features.shape)"
      ],
      "metadata": {
        "id": "nQT45TGC4xR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GraphConvLayer(layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_units,\n",
        "        dropout_rate=0.2,\n",
        "        aggregation_type=\"mean\",\n",
        "        combination_type=\"concat\",\n",
        "        normalize=False,\n",
        "        *args,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "        self.aggregation_type = aggregation_type\n",
        "        self.combination_type = combination_type\n",
        "        self.normalize = normalize\n",
        "\n",
        "        self.ffn_prepare = create_ffn(hidden_units, dropout_rate)\n",
        "        if self.combination_type == \"gated\":\n",
        "            self.update_fn = layers.GRU(\n",
        "                units=hidden_units,\n",
        "                activation=\"tanh\",\n",
        "                recurrent_activation=\"sigmoid\",\n",
        "                dropout=dropout_rate,\n",
        "                return_state=True,\n",
        "                recurrent_dropout=dropout_rate,\n",
        "            )\n",
        "        else:\n",
        "            self.update_fn = create_ffn(hidden_units, dropout_rate)\n",
        "\n",
        "    def prepare(self, node_repesentations, weights=None):\n",
        "        # node_repesentations shape is [num_edges, embedding_dim].\n",
        "        messages = self.ffn_prepare(node_repesentations)\n",
        "        if weights is not None:\n",
        "            messages = messages * tf.expand_dims(weights, -1)\n",
        "        return messages\n",
        "\n",
        "    def aggregate(self, node_indices, neighbour_messages, node_repesentations):\n",
        "        # node_indices shape is [num_edges].\n",
        "        # neighbour_messages shape: [num_edges, representation_dim].\n",
        "        # node_repesentations shape is [num_nodes, representation_dim]\n",
        "        num_nodes = node_repesentations.shape[0]\n",
        "        if self.aggregation_type == \"sum\":\n",
        "            aggregated_message = tf.math.unsorted_segment_sum(\n",
        "                neighbour_messages, node_indices, num_segments=num_nodes\n",
        "            )\n",
        "        elif self.aggregation_type == \"mean\":\n",
        "            aggregated_message = tf.math.unsorted_segment_mean(\n",
        "                neighbour_messages, node_indices, num_segments=num_nodes\n",
        "            )\n",
        "        elif self.aggregation_type == \"max\":\n",
        "            aggregated_message = tf.math.unsorted_segment_max(\n",
        "                neighbour_messages, node_indices, num_segments=num_nodes\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid aggregation type: {self.aggregation_type}.\")\n",
        "\n",
        "        return aggregated_message\n",
        "\n",
        "    def update(self, node_repesentations, aggregated_messages):\n",
        "        # node_repesentations shape is [num_nodes, representation_dim].\n",
        "        # aggregated_messages shape is [num_nodes, representation_dim].\n",
        "        if self.combination_type == \"gru\":\n",
        "            # Create a sequence of two elements for the GRU layer.\n",
        "            h = tf.stack([node_repesentations, aggregated_messages], axis=1)\n",
        "        elif self.combination_type == \"concat\":\n",
        "            # Concatenate the node_repesentations and aggregated_messages.\n",
        "            h = tf.concat([node_repesentations, aggregated_messages], axis=1)\n",
        "        elif self.combination_type == \"add\":\n",
        "            # Add node_repesentations and aggregated_messages.\n",
        "            h = node_repesentations + aggregated_messages\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid combination type: {self.combination_type}.\")\n",
        "\n",
        "        # Apply the processing function.\n",
        "        node_embeddings = self.update_fn(h)\n",
        "        if self.combination_type == \"gru\":\n",
        "            node_embeddings = tf.unstack(node_embeddings, axis=1)[-1]\n",
        "\n",
        "        if self.normalize:\n",
        "            node_embeddings = tf.nn.l2_normalize(node_embeddings, axis=-1)\n",
        "        return node_embeddings\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"Process the inputs to produce the node_embeddings.\n",
        "\n",
        "        inputs: a tuple of three elements: node_repesentations, edges, edge_weights.\n",
        "        Returns: node_embeddings of shape [num_nodes, representation_dim].\n",
        "        \"\"\"\n",
        "\n",
        "        node_repesentations, edges, edge_weights = inputs\n",
        "        # Get node_indices (source) and neighbour_indices (target) from edges.\n",
        "        node_indices, neighbour_indices = edges[0], edges[1]\n",
        "        # neighbour_repesentations shape is [num_edges, representation_dim].\n",
        "        neighbour_repesentations = tf.gather(node_repesentations, neighbour_indices)\n",
        "\n",
        "        # Prepare the messages of the neighbours.\n",
        "        neighbour_messages = self.prepare(neighbour_repesentations, edge_weights)\n",
        "        # Aggregate the neighbour messages.\n",
        "        aggregated_messages = self.aggregate(\n",
        "            node_indices, neighbour_messages, node_repesentations\n",
        "        )\n",
        "        # Update the node embedding with the neighbour messages.\n",
        "        return self.update(node_repesentations, aggregated_messages)"
      ],
      "metadata": {
        "id": "hzOBZQOM48TJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GNNNodeClassifier(tf.keras.Model):\n",
        "    def __init__(\n",
        "        self,\n",
        "        graph_info,\n",
        "        num_classes,\n",
        "        hidden_units,\n",
        "        aggregation_type=\"sum\",\n",
        "        combination_type=\"concat\",\n",
        "        dropout_rate=0.2,\n",
        "        normalize=True,\n",
        "        *args,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "        # Unpack graph_info to three elements: node_features, edges, and edge_weight.\n",
        "        node_features, edges, edge_weights = graph_info\n",
        "        self.node_features = node_features\n",
        "        self.edges = edges\n",
        "        self.edge_weights = edge_weights\n",
        "        # Set edge_weights to ones if not provided.\n",
        "        if self.edge_weights is None:\n",
        "            self.edge_weights = tf.ones(shape=edges.shape[1])\n",
        "        # Scale edge_weights to sum to 1.\n",
        "        self.edge_weights = self.edge_weights / tf.math.reduce_sum(self.edge_weights)\n",
        "\n",
        "        # Create a process layer.\n",
        "        self.preprocess = create_ffn(hidden_units, dropout_rate, name=\"preprocess\")\n",
        "        # Create the first GraphConv layer.\n",
        "        self.conv1 = GraphConvLayer(\n",
        "            hidden_units,\n",
        "            dropout_rate,\n",
        "            aggregation_type,\n",
        "            combination_type,\n",
        "            normalize,\n",
        "            name=\"graph_conv1\",\n",
        "        )\n",
        "        # Create the second GraphConv layer.\n",
        "        self.conv2 = GraphConvLayer(\n",
        "            hidden_units,\n",
        "            dropout_rate,\n",
        "            aggregation_type,\n",
        "            combination_type,\n",
        "            normalize,\n",
        "            name=\"graph_conv2\",\n",
        "        )\n",
        "        # Create a postprocess layer.\n",
        "        self.postprocess = create_ffn(hidden_units, dropout_rate, name=\"postprocess\")\n",
        "        # Create a compute logits layer.\n",
        "        self.compute_logits = layers.Dense(units=num_classes, name=\"logits\")\n",
        "\n",
        "    def call(self, input_node_indices):\n",
        "        # Preprocess the node_features to produce node representations.\n",
        "        x = self.preprocess(self.node_features)\n",
        "        # Apply the first graph conv layer.\n",
        "        x1 = self.conv1((x, self.edges, self.edge_weights))\n",
        "        # Skip connection.\n",
        "        x = x1 + x\n",
        "        # Apply the second graph conv layer.\n",
        "        x2 = self.conv2((x, self.edges, self.edge_weights))\n",
        "        # Skip connection.\n",
        "        x = x2 + x\n",
        "        # Postprocess node embedding.\n",
        "        x = self.postprocess(x)\n",
        "        # Fetch node embeddings for the input node_indices.\n",
        "        node_embeddings = tf.gather(x, input_node_indices)\n",
        "        # Compute logits\n",
        "        return self.compute_logits(node_embeddings)"
      ],
      "metadata": {
        "id": "jmAHSVDg5Mox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gnn_model = GNNNodeClassifier(\n",
        "    graph_info=graph_info,\n",
        "    num_classes=num_classes,\n",
        "    hidden_units=hidden_units,\n",
        "    dropout_rate=dropout_rate,\n",
        "    name=\"gnn_model\",\n",
        ")\n",
        "\n",
        "print(\"GNN output shape:\", gnn_model([1, 10, 100]))\n",
        "\n",
        "gnn_model.summary()"
      ],
      "metadata": {
        "id": "FB5jlVxH5QtB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = train_data.paper_id.to_numpy()\n",
        "history = run_experiment(gnn_model, x_train, y_train)"
      ],
      "metadata": {
        "id": "luEGs_xf5TYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_learning_curves(history)"
      ],
      "metadata": {
        "id": "xy4cj3Mt6N6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test = test_data.paper_id.to_numpy()\n",
        "_, test_accuracy = gnn_model.evaluate(x=x_test, y=y_test, verbose=0)\n",
        "print(f\"Test accuracy: {round(test_accuracy * 100, 2)}%\")"
      ],
      "metadata": {
        "id": "gp9eI-gf6VTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First we add the N new_instances as nodes to the graph\n",
        "# by appending the new_instance to node_features.\n",
        "num_nodes = node_features.shape[0]\n",
        "new_node_features = np.concatenate([node_features, new_instances])\n",
        "# Second we add the M edges (citations) from each new node to a set\n",
        "# of existing nodes in a particular subject\n",
        "new_node_indices = [i + num_nodes for i in range(num_classes)]\n",
        "new_citations = []\n",
        "for subject_idx, group in papers.groupby(\"subject\"):\n",
        "    subject_papers = list(group.paper_id)\n",
        "    # Select random x papers specific subject.\n",
        "    selected_paper_indices1 = np.random.choice(subject_papers, 5)\n",
        "    # Select random y papers from any subject (where y < x).\n",
        "    selected_paper_indices2 = np.random.choice(list(papers.paper_id), 2)\n",
        "    # Merge the selected paper indices.\n",
        "    selected_paper_indices = np.concatenate(\n",
        "        [selected_paper_indices1, selected_paper_indices2], axis=0\n",
        "    )\n",
        "    # Create edges between a citing paper idx and the selected cited papers.\n",
        "    citing_paper_indx = new_node_indices[subject_idx]\n",
        "    for cited_paper_idx in selected_paper_indices:\n",
        "        new_citations.append([citing_paper_indx, cited_paper_idx])\n",
        "\n",
        "new_citations = np.array(new_citations).T\n",
        "new_edges = np.concatenate([edges, new_citations], axis=1)"
      ],
      "metadata": {
        "id": "7pOchTaG6X8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Original node_features shape:\", gnn_model.node_features.shape)\n",
        "print(\"Original edges shape:\", gnn_model.edges.shape)\n",
        "gnn_model.node_features = new_node_features\n",
        "gnn_model.edges = new_edges\n",
        "gnn_model.edge_weights = tf.ones(shape=new_edges.shape[1])\n",
        "print(\"New node_features shape:\", gnn_model.node_features.shape)\n",
        "print(\"New edges shape:\", gnn_model.edges.shape)\n",
        "\n",
        "logits = gnn_model.predict(tf.convert_to_tensor(new_node_indices))\n",
        "probabilities = keras.activations.softmax(tf.convert_to_tensor(logits)).numpy()\n",
        "display_class_probabilities(probabilities)"
      ],
      "metadata": {
        "id": "mfPtmVMz6bfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H1KI2hzn6eiN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}